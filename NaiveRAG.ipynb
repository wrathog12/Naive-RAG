{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7532fc1d",
   "metadata": {},
   "source": [
    "### ðŸ§  What this code does (Imports)\n",
    "\n",
    "- Imports all essential LangChain modules to build a RAG (Retrieval-Augmented Generation) pipeline:\n",
    "\n",
    "| Module | Purpose |\n",
    "|--------|---------|\n",
    "| `PromptTemplate` | Creates structured prompts for LLMs. |\n",
    "| `RetrievalQA` | Combines retriever + LLM for question answering. |\n",
    "| `HuggingFaceEmbeddings` | Loads embedding models for vectorization. |\n",
    "| `FAISS` | Vectorstore for fast similarity search. |\n",
    "| `PyPDFLoader`, `DirectoryLoader` | Loads documents (PDFs or others) from disk. |\n",
    "| `RecursiveCharacterTextSplitter` | Splits documents into manageable text chunks. |\n",
    "| `CTransformers` | Loads and runs local LLMs (like LLaMA) efficiently. |\n",
    "\n",
    "âœ… Together, these enable document ingestion, embedding, retrieval, and generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceaa129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import CTransformers\n",
    "import os\n",
    "import json\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32248d8",
   "metadata": {},
   "source": [
    "### ðŸ“˜ How to Load JSON Files\n",
    "\n",
    "1. **Put JSON files** in a folder, e.g. `data/`.\n",
    "\n",
    "2. **Set the folder path** in code:\n",
    "```python\n",
    "extracted_data = load_json(\"data/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640c7b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_json(data_path):\n",
    "    documents = []\n",
    "    for filename in os.listdir(data_path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(data_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "                # Adjust this logic depending on your JSON structure\n",
    "                text = data.get(\"content\") or data.get(\"text\") or json.dumps(data)\n",
    "                documents.append(Document(page_content=text, metadata={\"source\": filename}))\n",
    "    \n",
    "    return documents\n",
    "\n",
    "extracted_data = load_json(\"data/\")\n",
    "print(f\"Loaded {len(extracted_data)} documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a30721",
   "metadata": {},
   "source": [
    "### ðŸ”¹ What this does\n",
    "\n",
    "Splits long documents into smaller text chunks (max 500 characters with 20 overlap) for better embedding and retrieval.\n",
    "\n",
    "### ðŸ”§ How to tweak\n",
    "\n",
    "- `chunk_size=500`: Increase for fewer, larger chunks (faster, but less precise).\n",
    "- `chunk_overlap=20`: Increase to preserve more context between chunks (helps answer long, context-heavy queries).\n",
    "\n",
    "Example:\n",
    "```python\n",
    "RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbc165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks\n",
    "\n",
    "text_chunks = text_split(extracted_data)\n",
    "print(f\"Number of chunks: {len(text_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28146e0",
   "metadata": {},
   "source": [
    "### ðŸ§  What this code does\n",
    "\n",
    "1. **Loads a sentence embedding model** from Hugging Face (`all-MiniLM-L6-v2`) â€” a fast, lightweight transformer for encoding text into vectors.\n",
    "\n",
    "2. **Encodes the query** `\"Hello world\"` into a dense vector using:\n",
    "```python\n",
    "embeddings.embed_query(\"Hello world\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bac431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings\n",
    "\n",
    "embeddings = download_hugging_face_embeddings()\n",
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(f\"Query Embedding Length: {len(query_result)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bfaa70",
   "metadata": {},
   "source": [
    "### ðŸ§  What this code does (FAISS Vectorstore)\n",
    "\n",
    "- Creates a FAISS index using the text chunks and embeddings.\n",
    "- Searches for the top 3 most similar chunks to your query.\n",
    "- Returns those relevant documents for downstream processing.\n",
    "\n",
    "âœ… FAISS is fast, scalable, and ideal for local vector search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec0b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = FAISS.from_texts([t.page_content for t in text_chunks], embeddings)\n",
    "\n",
    "query = \"What are allergies?\"\n",
    "docs = docsearch.similarity_search(query, k=3)\n",
    "print(\"Top 3 Documents for the Query:\", docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf637586",
   "metadata": {},
   "source": [
    "### ðŸ§  What this code does (Prompt Template)\n",
    "\n",
    "- Defines how the model should structure its answer.\n",
    "- Injects retrieved text into the `context`, and appends the `question`.\n",
    "- Ensures clear, helpful, and controlled answers â€” no hallucination.\n",
    "\n",
    "âœ… You can customize the tone, verbosity, or format by editing this template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdfd423",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1971e6cd",
   "metadata": {},
   "source": [
    "### ðŸ§  What this code does (LLM Loading with CTransformers)\n",
    "\n",
    "- Loads a local quantized `.bin` model using `CTransformers`.\n",
    "- Supports low-resource environments (runs on CPU).\n",
    "- `max_new_tokens` controls response length.\n",
    "- `temperature` affects creativity (lower = more factual).\n",
    "\n",
    "âœ… Make sure your `.bin` model is compatible and placed in the right path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214f4c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = CTransformers(\n",
    "    model=\"model/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "    model_type=\"llama\",\n",
    "    config={'max_new_tokens': 512, 'temperature': 0.8}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1502e89",
   "metadata": {},
   "source": [
    "### ðŸ§  What this code does (QA Chain Setup)\n",
    "\n",
    "- Creates a `RetrievalQA` chain with:\n",
    "  - a retriever (`FAISS`)\n",
    "  - a language model (`LLaMA`)\n",
    "  - a custom prompt template.\n",
    "- Retrieves top 2 matching chunks.\n",
    "- Returns both the answer and source documents used.\n",
    "\n",
    "âœ… Change `k` to return more/less context. Adjust `chain_type` for different behaviors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f079d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(search_kwargs={'k': 2}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7e73e1",
   "metadata": {},
   "source": [
    "### ðŸ§  What this code does (Interactive Q&A)\n",
    "\n",
    "- Starts a loop to take user input as a query.\n",
    "- Uses the QA chain to generate an answer with retrieved context.\n",
    "- Displays the result and repeats until you type `'exit'`.\n",
    "\n",
    "âœ… Great for testing and running your RAG system in the terminal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7061d1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        user_input = input(f\"\\nInput your question (type 'exit' to stop): \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"\\nExiting chat... Goodbye!\")\n",
    "            break\n",
    "\n",
    "        result = qa({\"query\": user_input})\n",
    "        response = result[\"result\"]\n",
    "\n",
    "        print(f\"\\nUser Question: {user_input}\")\n",
    "        print(f\"Bot Response: {response}\\n\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nExiting chat...\")\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "one",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
